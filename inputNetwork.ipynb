{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file takes in a user-defined class with a certain name. Instantiating the network is simply for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkAnalyzer:\n",
    "\n",
    "    def __init__(self, model_architecture: nn.Module, amount_to_produce: int, success_criteria: callable = None, max_attempts: float = None):\n",
    "        \"\"\"\n",
    "        Initializes the NetworkAnalyzer class with the required model architecture and configuration.\n",
    "\n",
    "        Args:\n",
    "            model_architecture (nn.Module): A PyTorch neural network model class specifying the architecture.\n",
    "            amount_to_produce (int): Number of good neural networks to generate based on success criteria.\n",
    "            success_criteria (callable, optional): A user-defined callable or lambda function specifying the \n",
    "                                                   acceptable loss threshold for a successful model. Defaults to 1.0.\n",
    "            max_attempts (float, optional): Maximum number of attempts to produce a successful network, defaults\n",
    "                                            to 130% of `amount_to_produce` if not specified.\n",
    "        \"\"\"\n",
    "        self.model_architecture = model_architecture\n",
    "        self.amount_to_produce = amount_to_produce\n",
    "        self.success_criteria = success_criteria if success_criteria is not None else self.default_success_criteria()\n",
    "        self.max_attempts = max_attempts if max_attempts is not None else self.default_max_attempts()\n",
    "\n",
    "    def default_success_criteria(self):\n",
    "        \"\"\"\n",
    "        Provides a default success criteria if none is specified by the user.\n",
    "        The default is a loss value of 1.0.\n",
    "\n",
    "        Returns:\n",
    "            float: The default success criteria (loss value).\n",
    "        \"\"\"\n",
    "        return 1.0\n",
    "\n",
    "    def default_max_attempts(self):\n",
    "        \"\"\"\n",
    "        Sets the maximum number of attempts to produce successful networks to 130% of the amount requested,\n",
    "        if not specified by the user.\n",
    "\n",
    "        Returns:\n",
    "            int: The calculated maximum number of attempts.\n",
    "        \"\"\"\n",
    "        return int(self.amount_to_produce * 1.3)\n",
    "\n",
    "    def show_loss(self, epoch: int, loss_value: float):\n",
    "        \"\"\"\n",
    "        Prints the loss value for a specific epoch during training.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The current epoch number during training.\n",
    "            loss_value (float): The loss value for the current epoch.\n",
    "        \"\"\"\n",
    "        print(f\"Epoch [{epoch+1}], Loss: {loss_value:.4f}\")\n",
    "\n",
    "    def train_network(self, model: nn.Module, train_loader: DataLoader, num_epochs: int, optimizer, loss_fn: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Trains a single network model over a specified number of epochs.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The PyTorch neural network model to train.\n",
    "            train_loader (DataLoader): The DataLoader for the training dataset.\n",
    "            num_epochs (int): Number of epochs to train the model.\n",
    "            optimizer (Optimizer): The optimizer used to update model weights.\n",
    "            loss_fn (torch.nn.Module): The loss function used to compute the training loss.\n",
    "        \"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            for batch in train_loader:\n",
    "                input, target = batch\n",
    "                optimizer.zero_grad()           \n",
    "                output = model(input)           \n",
    "                loss = loss_fn(output, target)  \n",
    "                loss.backward()                 \n",
    "                optimizer.step()                \n",
    "            self.show_loss(epoch, loss.item())  # Print loss per epoch\n",
    "            model.final_loss = loss.item()      # Save the final loss in the model\n",
    "\n",
    "    def generate_networks(self, train_loader: DataLoader, test_loader: DataLoader, num_epochs: int, loss_fn: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Generates multiple neural networks and trains them until a successful number of networks is produced,\n",
    "        based on the success criteria and the maximum number of allowed attempts.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader): The DataLoader for the training dataset.\n",
    "            test_loader (DataLoader): The DataLoader for the testing dataset (unused here but included for completeness).\n",
    "            num_epochs (int): The number of epochs for which each network should be trained.\n",
    "            loss_fn (torch.nn.Module): The loss function to use during training.\n",
    "        \"\"\"\n",
    "        self.models = []  # List to store successfully trained models\n",
    "\n",
    "        # Ensure directories for saving networks exist\n",
    "        working_dir = \"Working Networks\"\n",
    "        broken_dir = \"Broken Networks\"\n",
    "\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.makedirs(working_dir)  # Create directory if it doesn't exist\n",
    "        if not os.path.exists(broken_dir):\n",
    "            os.makedirs(broken_dir)\n",
    "\n",
    "\n",
    "        attempt = 0  # Counter for total attempts\n",
    "        success_count = 0  # Counter for successful networks\n",
    "\n",
    "        while success_count < self.amount_to_produce:\n",
    "            print(f\"Training Network {attempt+1}\")\n",
    "\n",
    "            # Stop if maximum attempts are reached\n",
    "            if attempt >= self.max_attempts:\n",
    "                print(\"Error: Maximum number of attempts reached without meeting success criteria.\")\n",
    "                break\n",
    "\n",
    "            # Instantiate a new network\n",
    "            network_to_be = self.model_architecture()\n",
    "            optimizer = torch.optim.SGD(network_to_be.parameters(), lr=0.001, momentum=0.9)  # SGD optimizer\n",
    "            \n",
    "            # Train the network\n",
    "            self.train_network(network_to_be, train_loader, num_epochs, optimizer, loss_fn)\n",
    "\n",
    "            # Check if the network meets the success criteria\n",
    "            if network_to_be.final_loss <= self.success_criteria:\n",
    "                torch.save(network_to_be.state_dict(), os.path.join(working_dir, f'network_{attempt+1}.pt'))\n",
    "                self.models.append(network_to_be)  # Add successful model to the list\n",
    "                success_count += 1\n",
    "            else:\n",
    "                torch.save(network_to_be.state_dict(), os.path.join(broken_dir, f'network_{attempt+1}.pt'))\n",
    "\n",
    "            attempt += 1  # Increment attempt counter\n",
    "\n",
    "        if success_count == self.amount_to_produce:\n",
    "            print(f\"Successfully trained {success_count} networks.\")\n",
    "        else:\n",
    "            print(f\"{len(self.models)} successful networks created out of {self.amount_to_produce}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everyrthing past here is testing. Will be deleted before pushing on to main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Network 1\n",
      "Epoch 1/5\n",
      "Epoch [1], Loss: 1.8357\n",
      "Epoch 2/5\n",
      "Epoch [2], Loss: 1.5371\n",
      "Epoch 3/5\n",
      "Epoch [3], Loss: 1.4437\n",
      "Epoch 4/5\n",
      "Epoch [4], Loss: 1.4822\n",
      "Epoch 5/5\n",
      "Epoch [5], Loss: 1.3012\n",
      "Training Network 2\n",
      "Epoch 1/5\n",
      "Epoch [1], Loss: 2.0784\n",
      "Epoch 2/5\n",
      "Epoch [2], Loss: 1.6727\n",
      "Epoch 3/5\n",
      "Epoch [3], Loss: 1.8037\n",
      "Epoch 4/5\n",
      "Epoch [4], Loss: 1.4079\n",
      "Epoch 5/5\n",
      "Epoch [5], Loss: 1.2976\n",
      "Training Network 3\n",
      "Epoch 1/5\n",
      "Epoch [1], Loss: 2.0482\n",
      "Epoch 2/5\n",
      "Epoch [2], Loss: 1.4924\n",
      "Epoch 3/5\n",
      "Epoch [3], Loss: 1.3063\n",
      "Epoch 4/5\n",
      "Epoch [4], Loss: 1.3851\n",
      "Epoch 5/5\n",
      "Epoch [5], Loss: 0.9773\n",
      "Training Network 4\n",
      "Error: Maximum number of attempts reached without meeting success criteria.\n",
      "1 successful networks created out of 2.\n"
     ]
    }
   ],
   "source": [
    "analyzer = NetworkAnalyzer(Net,2, max_attempts=3)\n",
    "analyzer.generate_networks(trainloader, testloader, 5, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
